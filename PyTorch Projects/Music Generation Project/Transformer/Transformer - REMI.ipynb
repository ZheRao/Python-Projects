{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eccd9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28c273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc993740",
   "metadata": {},
   "source": [
    "# Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db14f0ab",
   "metadata": {},
   "source": [
    "## embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d910ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self,num_token,d_model):\n",
    "        super(Embedder,self).__init__()\n",
    "        self.embed = nn.Embedding(num_token,d_model)\n",
    "    def forward(self,x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ac7d8",
   "metadata": {},
   "source": [
    "## positional encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1977ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant \"positoinal encoding pe\" matrix with values dependant on \n",
    "        #  position and i\n",
    "        pe = torch.zeros(max_seq_len,d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,d_model,2):\n",
    "                pe[pos,i] = math.sin(pos/(10000** ((2*i)/d_model)))\n",
    "                pe[pos,i+1] = math.cos(pos/(10000** ((2*(i+1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # make embedding larger so that its more important\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        \n",
    "        # add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.nn.parameter.Parameter(self.pe[:,:seq_len], requires_grad=False).cuda()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398272a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9285336e",
   "metadata": {},
   "source": [
    "## MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d97b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        # claculate Q, K, V representation of the token\n",
    "        self.q_linear = nn.Linear(d_model,d_model)\n",
    "        self.v_linear = nn.Linear(d_model,d_model)\n",
    "        self.k_linear = nn.Linear(d_model,d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model,d_model)\n",
    "        \n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs,-1,self.h,self.d_k)\n",
    "        q = self.q_linear(q).view(bs,-1,self.h,self.d_k)\n",
    "        v = self.v_linear(v).view(bs,-1,self.h,self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * heads * sequence_len * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        scores = attention(q, k, v, self.d_k, None, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs,-1,self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # calculate attention\n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    scores = torch.matmul(q,k.transpose(2,-1)) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill_(mask == 0, -1e9)\n",
    "    \n",
    "    scores = nn.functional.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout:\n",
    "        scores = dropout(scores)\n",
    "    \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ded78",
   "metadata": {},
   "source": [
    "# Feed-forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb2795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ff = 2048, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model,d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff,d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.dropout(self.relu(self.linear1(x)))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955e89c",
   "metadata": {},
   "source": [
    "# Layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c67333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.size = d_model\n",
    "        \n",
    "        # create two learnable parameters to calibrate normalization\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self,x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1,keepdim=True))/(x.std(dim=-1,keepdim=True)+self.eps)+self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a3bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c9e6b56",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c492a",
   "metadata": {},
   "source": [
    "## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "127f9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build one encoder layer with one multi-head attention layer and one feed-forward layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,heads,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.norm1 = Norm(d_model)\n",
    "        self.norm2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x,mask): # input (seq_len, embedding_dim)\n",
    "        x2 = self.norm1(x) \n",
    "        x = x + self.dropout1(self.attn(x2,x2,x2,mask)) # \n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "# a decoder layer with two multi-head attention layers and one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,heads,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.norm1 = Norm(d_model)\n",
    "        self.norm2 = Norm(d_model)\n",
    "        self.norm3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn1 = MultiHeadAttention(heads,d_model)\n",
    "        self.attn2 = MultiHeadAttention(heads,d_model)\n",
    "        self.ff=FeedForward(d_model).cuda()\n",
    "        \n",
    "    def forward(self,x,e_outputs,src_mask,trg_mask):\n",
    "        x2 = self.norm1(x)\n",
    "        x = x + self.dropout1(self.attn1(x2,x2,x2,trg_mask))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.attn2(x2,e_outputs,e_outputs,src_mask))\n",
    "        x2 = self.norm3(x)\n",
    "        x = x + self.dropout3(self.ff(x2))\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b18ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that generate multiple layers:\n",
    "def get_clones(module,N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "# encoder and decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,num_tokens,d_model,N,heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(num_tokens,d_model) # num_tokens is vocabulary size\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model,heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        \n",
    "    def forward(self,src,mask): # src size (seq_len,)\n",
    "        x = self.embed(src) # embed size (seq_len,embedding dimension) note: sometimes use hidden_dimension\n",
    "        x = self.pe(x) \n",
    "        for i in range(N):\n",
    "            x = self.layers[i](x,mask) # go though N layers, input (seq_len,embedding_dim), output \n",
    "        return self.norm(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,num_tokens,d_model,N,heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(num_tokens,d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(DecoderLayer(d_model,heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    \n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x,e_outputs,src_mask,trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0bbb65",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01478011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,src_num_tokens,trg_num_tokens,d_model,N,heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_num_tokens,d_model,N,heads)\n",
    "        #self.decoder = Decoder(trg_num_tokens,d_model,N,heads)\n",
    "        self.out = nn.Linear(d_model,trg_num_tokens)\n",
    "    \n",
    "    def forward(self, src, src_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        #d_output = self.decoder(trg,e_outputs,src_mask,trg_mask)\n",
    "        output = self.out(e_outputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b883fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de275f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a4237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48316799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a854fa7d",
   "metadata": {},
   "source": [
    "# customized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f2267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customizedDataset(data.Dataset):\n",
    "    def __init__(self,npy_path):\n",
    "        # read in data\n",
    "        self.data = np.load(npy_path)\n",
    "        print(\"Dataset shape: \", self.data.shape)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        sequence = self.data[index]\n",
    "        input_seq = sequence[:-1]\n",
    "        output_seq = sequence[1:]\n",
    "        return (input_seq,output_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88970b72",
   "metadata": {},
   "source": [
    "## dataloders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b48bad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (24613, 512)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = customizedDataset(\"Customized Dataset - Pop/Train/customized_training_512.npy\")\n",
    "train_dataloader = data.DataLoader(train_dataset,batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f303a6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (2272, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_dataset = customizedDataset(\"Customized Dataset - Pop/Val/customized_val_512.npy\")\n",
    "\n",
    "set1 = list(range(0,len(val_dataset),2))\n",
    "set2 = list(range(1,len(val_dataset),2))\n",
    "#set3 = list(range(2,len(val_dataset),4))\n",
    "#set4 = list(range(3,len(val_dataset),4))\n",
    "\n",
    "val1 = data.Subset(val_dataset,set1)\n",
    "val2 = data.Subset(val_dataset,set2)\n",
    "#val3 = data.Subset(val_dataset,set3)\n",
    "#val4 = data.Subset(val_dataset,set4)\n",
    "\n",
    "size=2\n",
    "\n",
    "val_dataloader1 = data.DataLoader(val1,batch_size=size,shuffle=True)\n",
    "val_dataloader2 = data.DataLoader(val2,batch_size=size,shuffle=True)\n",
    "#val_dataloader3 = data.DataLoader(val3,batch_size=size,shuffle=True)\n",
    "#val_dataloader4 = data.DataLoader(val4,batch_size=size,shuffle=True)\n",
    "\n",
    "val_dataloader = [val_dataloader1,\n",
    "               val_dataloader2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bef52036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 511])\n",
      "torch.Size([2, 511])\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "X_train = next(iter(train_dataloader))\n",
    "print(X_train[0].shape) # input seq\n",
    "print(X_train[1].shape) # output seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b81e777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 511])\n",
      "torch.Size([2, 511])\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "X_val = next(iter(val_dataloader[0]))\n",
    "print(X_val[0].shape) # input seq\n",
    "print(X_val[1].shape) # output seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4259d4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12307"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef4aa2",
   "metadata": {},
   "source": [
    "# Train and Validation Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7054629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(seq):\n",
    "    size = seq.size(1)\n",
    "    nopeak_mask = np.triu(np.ones((1,size,size)),k=1).astype(\"uint8\")\n",
    "    nopeak_mask = (torch.from_numpy(nopeak_mask)==0)\n",
    "    return nopeak_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "860414b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_pad(seq,paded_seq):\n",
    "    return (seq != padded_seq).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90a51719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros((3,3),dtype=torch.long)\n",
    "a[0,0] = 1\n",
    "a[1,1] = 1\n",
    "a[2,2] = 1\n",
    "b = torch.ones((3,3),dtype=torch.long)\n",
    "b = b * 3\n",
    "c = a & b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "029bc1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,val_dataloader,device,loss_fn,num_vocab):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    steps = 0 # keep track of number of batches \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            steps += 1\n",
    "            input_seq, output_seq = batch\n",
    "            input_seq, output_seq = input_seq.long().to(device), output_seq.long().to(device)\n",
    "            output = model(input_seq,None) #shape (batch,seq_len,num_vocab)\n",
    "            output_flatten = output.view(-1,num_vocab) #shape (x,num_vocab)\n",
    "            # output shape (batch,seq_len)\n",
    "            loss +=  loss_fn(output_flatten,output_seq.view(-1)).item()\n",
    "    loss = loss / steps\n",
    "    print(f\"Validation loss: {loss:.6f}\\n\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1282de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def train(model,train_dataloader,val_dataloader,ep,print_frequency,loss_fn,\n",
    "          optimizer,device,run,grad_clip=1,scheduler=None,\n",
    "          num_vocab = 218,kind=\"Pop\"):\n",
    "    model.train()\n",
    "    val_best = float(\"inf\")\n",
    "    \n",
    "    # total number of training steps:\n",
    "    num_steps = ep * len(train_dataloader) # len(train_dataloader) is how many batches for 1 epoch\n",
    "    progress = tqdm(range(num_steps))\n",
    "    \n",
    "    # calculate how often print the result\n",
    "    print_every = math.floor(num_steps/print_frequency) # this is the # of batches processed before print\n",
    "    \n",
    "    \n",
    "    # initialize\n",
    "    steps = 0\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    current_loss = 0\n",
    "    \n",
    "    for e in range(ep):\n",
    "        for batch in train_dataloader:\n",
    "            steps += 1\n",
    "            input_seq, output_seq = batch\n",
    "            input_seq, output_seq = input_seq.long().to(device), output_seq.long().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq,None) # don't need mask, there is no padding\n",
    "            output_flatten = output.view(-1,num_vocab)\n",
    "            loss = loss_fn(output_flatten,output_seq.view(-1))\n",
    "            current_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),grad_clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            progress.update(1)\n",
    "            \n",
    "            # print \n",
    "            if steps % print_every == 0:\n",
    "                current_loss = current_loss / print_every\n",
    "                print(f\"EPOCHS  : {e+1}/{ep} Loss: {current_loss:.6f}\")\n",
    "                train_loss.append(current_loss)\n",
    "                current_loss = 0\n",
    "                \n",
    "                # validation\n",
    "                which_val = np.random.randint(0,1)\n",
    "                current_val_loss = evaluate(model,val_dataloader[which_val],device,loss_fn,num_vocab)\n",
    "                val_loss.append(current_val_loss)\n",
    "                \n",
    "                # save model\n",
    "                if current_val_loss < val_best:\n",
    "                    val_best = current_val_loss\n",
    "                    torch.save(model.state_dict(),f\"TransformerWeights/{kind}Transformer-run-{run}-val_loss_BEST.pth\")\n",
    "                else:\n",
    "                    torch.save(model.state_dict(),f\"TransformerWeights/{kind}Transformer-run-{run}-val_loss_LAST.pth\")\n",
    "                \n",
    "            model.train()\n",
    "        if scheduler and (e + (ep*(run-1)) <= 3): # less than 8 epochs\n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "    return train_loss, val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98e5e724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010\n"
     ]
    }
   ],
   "source": [
    "a = 0.01\n",
    "print(f\"{a:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec32b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51680fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a4fd9bb",
   "metadata": {},
   "source": [
    "# Train! - Complex Pop Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "097360d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary size\n",
    "from miditok import REMI, get_midi_programs\n",
    "from miditoolkit import MidiFile\n",
    "tokenizer = REMI()\n",
    "num_vocab = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41304459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c2b8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "d_model = 1024\n",
    "N = 12 # num of transformer encoder layers within transformer encoder\n",
    "heads = 8\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "src_num_tokens = num_vocab\n",
    "trg_num_tokens = num_vocab\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c9b601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1975bd1b",
   "metadata": {},
   "source": [
    "## first 20 epochs - 2 layer, 4 head, batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26ba856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-06.\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-6 # learning rate\n",
    "PopTransformer = Transformer(src_num_tokens,trg_num_tokens,d_model,N,heads).cuda()\n",
    "optimizer = torch.optim.Adam(PopTransformer.parameters(),lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,1,gamma=0.25,verbose=True)\n",
    "\n",
    "# initializes the parameters with a range of values that stops the singal fading or \n",
    "#  getting too big\n",
    "for p in PopTransformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce84bae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d24f830c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a44af0819b546ffa596365a91cbfcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS  : 1/5 Loss: 3.319919\n",
      "Validation loss: 2.893273\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 2.926236\n",
      "Validation loss: 2.823462\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 2.861809\n",
      "Validation loss: 2.782711\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 2.834690\n",
      "Validation loss: 2.752697\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 2.780121\n",
      "Validation loss: 2.666330\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 2.701588\n",
      "Validation loss: 2.579406\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 2.623405\n",
      "Validation loss: 2.448453\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 2.484658\n",
      "Validation loss: 2.214432\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 2.315582\n",
      "Validation loss: 2.022099\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 2.181854\n",
      "Validation loss: 1.836695\n",
      "\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "EPOCHS  : 2/5 Loss: 2.064630\n",
      "Validation loss: 1.735756\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 1.982906\n",
      "Validation loss: 1.605459\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 1.888807\n",
      "Validation loss: 1.488358\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 1.790541\n",
      "Validation loss: 1.379290\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 1.701470\n",
      "Validation loss: 1.280403\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 1.623118\n",
      "Validation loss: 1.187325\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 1.542547\n",
      "Validation loss: 1.106253\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 1.471454\n",
      "Validation loss: 1.027003\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 1.402719\n",
      "Validation loss: 0.957129\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 1.338337\n",
      "Validation loss: 0.891732\n",
      "\n",
      "Adjusting learning rate of group 0 to 2.5000e-06.\n",
      "EPOCHS  : 3/5 Loss: 1.283751\n",
      "Validation loss: 0.846822\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 1.251365\n",
      "Validation loss: 0.817578\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 1.224160\n",
      "Validation loss: 0.783923\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 1.193247\n",
      "Validation loss: 0.757966\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 1.169084\n",
      "Validation loss: 0.730187\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 1.142400\n",
      "Validation loss: 0.704862\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 1.115665\n",
      "Validation loss: 0.674946\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 1.091624\n",
      "Validation loss: 0.654240\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 1.068041\n",
      "Validation loss: 0.627364\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 1.046174\n",
      "Validation loss: 0.601424\n",
      "\n",
      "Adjusting learning rate of group 0 to 1.2500e-06.\n",
      "EPOCHS  : 4/5 Loss: 1.019199\n",
      "Validation loss: 0.585980\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 1.015095\n",
      "Validation loss: 0.572857\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.998950\n",
      "Validation loss: 0.561881\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.988639\n",
      "Validation loss: 0.550171\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.973782\n",
      "Validation loss: 0.539650\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.966180\n",
      "Validation loss: 0.528920\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.953109\n",
      "Validation loss: 0.515953\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.943368\n",
      "Validation loss: 0.505508\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.930647\n",
      "Validation loss: 0.492859\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.920386\n",
      "Validation loss: 0.481647\n",
      "\n",
      "Adjusting learning rate of group 0 to 6.2500e-07.\n",
      "EPOCHS  : 5/5 Loss: 0.907999\n",
      "Validation loss: 0.474579\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.903420\n",
      "Validation loss: 0.468103\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.893162\n",
      "Validation loss: 0.461041\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.887776\n",
      "Validation loss: 0.452576\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.877350\n",
      "Validation loss: 0.442910\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.870682\n",
      "Validation loss: 0.430908\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.862296\n",
      "Validation loss: 0.415523\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.848097\n",
      "Validation loss: 0.396455\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.827828\n",
      "Validation loss: 0.377821\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.810116\n",
      "Validation loss: 0.360703\n",
      "\n",
      "Adjusting learning rate of group 0 to 3.1250e-07.\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train(PopTransformer,train_dataloader,val_dataloader,5,50,\n",
    "      loss_fn,optimizer,'cuda',1,grad_clip=1,scheduler=scheduler,\n",
    "                             num_vocab=num_vocab,kind=\"ModifiedSuperComplexPop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5637b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"TransformerLoss/ModifiedSuperComplexPopTransformer_run1_trainloss.npy\",np.array(train_loss))\n",
    "np.save(\"TransformerLoss/ModifiedSuperComplexPopTransformer_run1_valloss.npy\",np.array(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d97eff05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PopTransformer.load_state_dict(torch.load(\"TransformerWeights/ModifiedSuperComplexPopTransformer-run-1-val_loss_0.36.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1fc162f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4956a53837ff44ff89f87c4628f5d1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS  : 1/5 Loss: 0.786824\n",
      "Validation loss: 0.335687\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 0.758714\n",
      "Validation loss: 0.307433\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 0.732214\n",
      "Validation loss: 0.283259\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 0.706173\n",
      "Validation loss: 0.256800\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 0.676352\n",
      "Validation loss: 0.233120\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 0.649305\n",
      "Validation loss: 0.210312\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 0.610089\n",
      "Validation loss: 0.185629\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 0.578247\n",
      "Validation loss: 0.164072\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 0.551488\n",
      "Validation loss: 0.149189\n",
      "\n",
      "EPOCHS  : 1/5 Loss: 0.524380\n",
      "Validation loss: 0.134440\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 0.502593\n",
      "Validation loss: 0.123661\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 0.482453\n",
      "Validation loss: 0.114390\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 0.467371\n",
      "Validation loss: 0.105669\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 0.442775\n",
      "Validation loss: 0.097588\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 0.428977\n",
      "Validation loss: 0.091310\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 0.418423\n",
      "Validation loss: 0.085575\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 0.398248\n",
      "Validation loss: 0.080494\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 0.390843\n",
      "Validation loss: 0.076454\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 0.374392\n",
      "Validation loss: 0.071975\n",
      "\n",
      "EPOCHS  : 2/5 Loss: 0.362963\n",
      "Validation loss: 0.068936\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 0.355849\n",
      "Validation loss: 0.065236\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 0.336678\n",
      "Validation loss: 0.061870\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 0.331605\n",
      "Validation loss: 0.059181\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 0.319074\n",
      "Validation loss: 0.056152\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 0.312350\n",
      "Validation loss: 0.053212\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 0.300524\n",
      "Validation loss: 0.050969\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 0.294125\n",
      "Validation loss: 0.048317\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 0.284671\n",
      "Validation loss: 0.046893\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 0.277193\n",
      "Validation loss: 0.044704\n",
      "\n",
      "EPOCHS  : 3/5 Loss: 0.273791\n",
      "Validation loss: 0.042314\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.261093\n",
      "Validation loss: 0.040978\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.257030\n",
      "Validation loss: 0.039341\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.249130\n",
      "Validation loss: 0.037300\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.244807\n",
      "Validation loss: 0.035895\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.240481\n",
      "Validation loss: 0.034638\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.229788\n",
      "Validation loss: 0.033080\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.223441\n",
      "Validation loss: 0.031641\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.223113\n",
      "Validation loss: 0.030639\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.215918\n",
      "Validation loss: 0.029457\n",
      "\n",
      "EPOCHS  : 4/5 Loss: 0.211284\n",
      "Validation loss: 0.028465\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.204244\n",
      "Validation loss: 0.026892\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.199463\n",
      "Validation loss: 0.026036\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.192610\n",
      "Validation loss: 0.024918\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.191874\n",
      "Validation loss: 0.024035\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.183915\n",
      "Validation loss: 0.023167\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.176369\n",
      "Validation loss: 0.022505\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.176195\n",
      "Validation loss: 0.021866\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.168899\n",
      "Validation loss: 0.021202\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.162675\n",
      "Validation loss: 0.020490\n",
      "\n",
      "EPOCHS  : 5/5 Loss: 0.163537\n",
      "Validation loss: 0.019817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = train(PopTransformer,train_dataloader,val_dataloader,5,50,\n",
    "      loss_fn,optimizer,'cuda',2,grad_clip=1,scheduler=scheduler,\n",
    "                             num_vocab=num_vocab,kind=\"ModifiedSuperComplexPop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "530cadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"TransformerLoss/ModifiedSuperComplexPopTransformer_run2_trainloss.npy\",np.array(train_loss))\n",
    "np.save(\"TransformerLoss/ModifiedSuperComplexPopTransformer_run2_valloss.npy\",np.array(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a2070a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39450225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1275bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3273b384",
   "metadata": {},
   "source": [
    "# Generate music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c1e091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(model,name,input_seq_len,output_seq_len,start=0):\n",
    "    # get tokenized remi representation of the music\n",
    "    tokenizer = REMI() # initialize tokenizer\n",
    "    folder = \"Generated Music/Input/\"\n",
    "    #midi = MidiFile(folder+name+\".mid\") # classical\n",
    "    midi = MidiFile(folder+name+\".midi\")\n",
    "    events_MIDI = tokenizer.midi_to_tokens(midi)\n",
    "    events_MIDI = events_MIDI[0]\n",
    "    \n",
    "    # find the last bar\n",
    "    while events_MIDI[start] != 1:\n",
    "        start -= 1\n",
    "    \n",
    "\n",
    "    # the sequence will start with a bar\n",
    "    input_seq = events_MIDI[start:start+input_seq_len]\n",
    "    #assert input_seq_len <= 512\n",
    "    print(\"input length: \",len(input_seq))\n",
    "    \n",
    "    \n",
    "    predicted_seq = [1]\n",
    "    \n",
    "    # find the last bar event from the input seqeunce and append it into predicted_seq\n",
    "    #position = len(input_seq) - 1\n",
    "    #while input_seq[position] != 1:\n",
    "        #position -= 1\n",
    "    #predicted_seq.extend(input_seq[position:])\n",
    "\n",
    "    # fed into model to obtain hidden state\n",
    "    input_seq = torch.tensor(input_seq,dtype=torch.long)\n",
    "    input_seq = input_seq.to(\"cuda\")\n",
    "    model.eval()\n",
    "    #print(input_seq)\n",
    "    \n",
    "    \n",
    "    #op = model(input_seq.unsqueeze(0),None)\n",
    "    #op = op.view(-1,218)\n",
    "    #op = nn.functional.softmax(op,dim=1)\n",
    "    #op = op.squeeze(0)\n",
    "    #predicted_events = op.argmax(1)\n",
    "    #predicted_events = torch.cat((torch.tensor([1],device=\"cuda\"),predicted_events))\n",
    "    #predicted_events = predicted_events.cpu().detach().numpy()\n",
    "    #print(predicted_events)\n",
    "    #return predicted_events\n",
    "    \n",
    "    #pred = [1]\n",
    "    #input [1,2,3,4]\n",
    "    #output [2,3,4,5]\n",
    "    #pred=[1,2]\n",
    "    #input [2,3,4,5]\n",
    "    #output [3,4,5,6]\n",
    "    #pred=[1,2,3]\n",
    "    \n",
    "    \n",
    "    for i in np.arange(output_seq_len):\n",
    "        # use model to predict the next output\n",
    "        #print(\"input: \",input_seq)\n",
    "        op = model(input_seq.unsqueeze(0),None) # seq_len, hidden_size\n",
    "        op = op.view(-1,218)\n",
    "        # the latest predicted event\n",
    "        op = nn.functional.softmax(op,dim=1)\n",
    "        #print(\"model output shape: \", op.shape)\n",
    "        op = op.squeeze(0)\n",
    "        op = op.argmax(1)\n",
    "        predicted_seq.append(op[-1].cpu().detach().numpy().item())\n",
    "        #print(len(input_seq))\n",
    "        #print(input_seq)\n",
    "        input_seq = input_seq[1:]\n",
    "        #print(len(input_seq))\n",
    "        #print(torch.tensor([op[-1]],device=\"cuda\"))\n",
    "        #print(input_seq)\n",
    "        input_seq = torch.cat((input_seq,torch.tensor([op[-1]],device=\"cuda\")))\n",
    "        #print(len(input_seq))\n",
    "        #print(input_seq)\n",
    "        #assert len(input_seq) == input_seq_len\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"squzzezed output shape: \", op.shape)\n",
    "        #beginning_event = op.argmax(1)[0]\n",
    "        #predicted_event = op.argmax(1)[-1]\n",
    "        #print(\"output: \",op.argmax(1))\n",
    "        #print(beginning_event)\n",
    "        #print(predicted_event)\n",
    "        #print(\"length of predicted event: \",predicted_event.shape)\n",
    "        #print(\"predicted_event \", predicted_event)\n",
    "        \n",
    "        \n",
    "        # append the event\n",
    "        #input_seq = torch.cat((input_seq,torch.tensor([predicted_event],device=\"cuda\")))\n",
    "        #input_seq = op # since appended the newest predicted event, remove the first event\n",
    "        #print(\"new input: \", input_seq)\n",
    "        #predicted_seq.append(predicted_event.cpu().detach().numpy().item())\n",
    "        #print(predicted_seq)\n",
    "    \n",
    "    #predicted_seq.extend(op[1:].cpu().detach().numpy())\n",
    "        \n",
    "    print(predicted_seq)\n",
    "    \n",
    "    return predicted_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03be36cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor(3)\n",
    "\n",
    "print(b.shape)\n",
    "a = torch.cat((a,torch.tensor([b])))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10cf18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "a = np.insert(a,0,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c6cf0",
   "metadata": {},
   "source": [
    "## initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3458625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary size\n",
    "from miditok import REMI, get_midi_programs\n",
    "from miditoolkit import MidiFile\n",
    "tokenizer = REMI()\n",
    "num_vocab = len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de1a0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "d_model = 1024\n",
    "N = 12 # num of transformer encoder layers within transformer encoder\n",
    "heads = 8\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "src_num_tokens = num_vocab\n",
    "trg_num_tokens = num_vocab\n",
    "\n",
    "transformer = Transformer(src_num_tokens,trg_num_tokens,d_model,N,heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b00120ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_state_dict(torch.load(\"TransformerWeights/SecondFinestComplexPopTransformer-run-3-val_loss_0.11.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3f85c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input length:  1024\n",
      "[1, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86, 108, 57, 187, 179, 71, 201, 76, 208, 43, 81, 86]\n"
     ]
    }
   ],
   "source": [
    "music = \"001\"\n",
    "#transformer = PopTransformer\n",
    "output_seq = generate_music(transformer.cuda(),music,1024,500,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e20ff6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1011"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43f10591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Music/FinestComplexPopTransformer/run-1-001-0.11.midi\n"
     ]
    }
   ],
   "source": [
    "model_name = \"FinestComplexPopTransformer\"\n",
    "run = \"1\"\n",
    "mode = \"0.11\"\n",
    "\n",
    "output_path = \"Generated Music/\"+model_name+\"/run-\"+run+\"-\"+music+\"-\"+mode+\".midi\"\n",
    "#music_path = \"Generated Music/Input/\" + music + \".mid\" # classical\n",
    "music_path = \"Generated Music/Input/\" + music + \".midi\"\n",
    "tokenizer = REMI()\n",
    "midi = MidiFile(music_path)\n",
    "output_midi = tokenizer.tokens_to_midi([output_seq],get_midi_programs(midi))\n",
    "\n",
    "output_midi.dump(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4467e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4de462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583331b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
