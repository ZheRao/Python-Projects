{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b289fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a23225f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if gpu is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0e5c2",
   "metadata": {},
   "source": [
    "# step1: define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29de132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use torch.nn module to define a model with \n",
    "# 1) two convolutional layers with output channel 16, 32 each with kernel 3 and stride 1\n",
    "# 2) relu activation after each convolutional layer\n",
    "# 3) max pooling with kernel 2\n",
    "# 4) a dropout layer with rate 10%\n",
    "# 5) a flattern layer\n",
    "# 6) a dense layer with 64 neurons\n",
    "# 7) a relu activation\n",
    "# 8) a drop out layer with rate 25%\n",
    "# 9) a dense layer with 10 neurons\n",
    "# 10) use softmax activation\n",
    "\n",
    "class myConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1) # can we choose what features?\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.do1 = nn.Dropout2d(0.1)\n",
    "        self.do2 = nn.Dropout(0.25)\n",
    "        self.lf1 = nn.Linear(4608, 64) # have to calculate the input size by hand?\n",
    "        self.lf2 = nn.Linear(64,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = self.do1(x)\n",
    "        x = torch.flatten(x,1) # not flatten the batch channel?\n",
    "        x = self.lf1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.do2(x)\n",
    "        x = self.lf2(x)\n",
    "        op = F.log_softmax(x,1) # have to exclude last channel for all functions?\n",
    "        return op              # don't need to specify the number of output classes?\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2597e",
   "metadata": {},
   "source": [
    "## define the training and testing routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c11a1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad() # set gradients to 0\n",
    "        pred_prob = model(X) # output probabilities for each class?\n",
    "        loss = F.nll_loss(pred_prob, y)\n",
    "        loss.backward() # compute gradient\n",
    "        optim.step() # upedate weights\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                    epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                    100. * b_i / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1cdf8064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval() # for making inference?\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad(): # not calculating gradients\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction=\"sum\").item() # reduction='sum'?\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "            loss /= len(test_dataloader.dataset)\n",
    "            print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                loss, success, len(test_dataloader.dataset),\n",
    "                100. * success / len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa22ae",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fb48187",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "                    datasets.MNIST('W://Study Material/Jupyter Notebook/Small Projects/MNIST - PyTorch',\n",
    "                                      train=True,transform=transforms.Compose([\n",
    "                                          transforms.ToTensor(), transforms.Normalize((0.1302,),\n",
    "                                                                                     (0.3069,))\n",
    "                                      ])), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65e16aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "                    datasets.MNIST('W://Study Material/Jupyter Notebook/Small Projects/MNIST - PyTorch',\n",
    "                                      train=False,transform=transforms.Compose([\n",
    "                                          transforms.ToTensor(), transforms.Normalize((0.1302,),\n",
    "                                                                                     (0.3069,))\n",
    "                                      ])), batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b953590",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = myConv()\n",
    "model.cuda()\n",
    "optimizer = optim.Adadelta(model.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87a7c127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.320377\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 2.232965\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.910860\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 1.584494\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 1.081735\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 1.003600\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.881128\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.769732\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.975686\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.612269\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.612677\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.752664\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.515795\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.829541\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.593325\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.706249\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.415628\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.516252\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.264734\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.295494\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 1.041159\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.248755\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.588695\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.323724\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.424682\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.246926\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.702430\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.234933\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.345835\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.552984\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.305311\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.386915\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.304578\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.650302\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.190960\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.343009\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.522811\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.304053\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.348726\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.144249\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.209308\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.121003\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.206269\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.092533\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.249691\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.334066\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.279585\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.289807\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.218300\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.403084\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.126687\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.492402\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.158763\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.379411\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.092904\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.274954\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.106338\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.151952\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.289675\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.380398\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.235243\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.190429\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.166053\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.283460\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.252847\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.189457\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.486306\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.162728\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.100933\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.269161\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.240486\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.261927\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.066105\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.233384\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.391233\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.226771\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.090938\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.287424\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.085982\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.089531\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.521502\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.447245\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.121030\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.233073\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.245000\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.133218\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.110209\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.196674\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.075132\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.309871\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.145173\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.561637\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.057561\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.143512\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.558830\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.105296\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.285273\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.239049\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.174564\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.058958\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.048162\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.322190\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.238644\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.053962\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.241715\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.174535\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.114903\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.094443\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.054098\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.208785\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.206231\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.202555\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.051948\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.065043\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.223163\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.199665\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.186657\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.253127\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.095628\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.175170\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.105051\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.297090\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.081430\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.375616\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.026241\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.076245\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.183421\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.164843\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.239220\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.293939\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.262835\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.041133\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.035085\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.155981\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.152999\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.097468\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.189463\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.121727\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.164743\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.194194\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.429871\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.169990\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.229435\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.042820\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.359785\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.142332\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.137180\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.149971\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.152061\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.138569\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.085962\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.264070\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.035347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.149566\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.310765\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.044413\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.087606\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.146554\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.019678\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.017864\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.300845\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.064873\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.026559\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.039999\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.020557\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.157173\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.092729\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.144155\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.106605\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.069951\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.178140\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.138642\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.013456\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.131879\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.050894\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.123206\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.140130\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.158477\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.109122\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.107792\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.024309\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.086187\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.037435\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.103177\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.055985\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.023969\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.059917\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.195217\n",
      "\n",
      "Test dataset: Overall Loss: 0.0031, Overall Accuracy: 489/10000 (5%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0044, Overall Accuracy: 975/10000 (10%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0059, Overall Accuracy: 1458/10000 (15%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0053, Overall Accuracy: 1945/10000 (19%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0068, Overall Accuracy: 2417/10000 (24%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0039, Overall Accuracy: 2906/10000 (29%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0033, Overall Accuracy: 3395/10000 (34%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0075, Overall Accuracy: 3876/10000 (39%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0046, Overall Accuracy: 4359/10000 (44%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0045, Overall Accuracy: 4842/10000 (48%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0010, Overall Accuracy: 5339/10000 (53%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0036, Overall Accuracy: 5830/10000 (58%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0019, Overall Accuracy: 6322/10000 (63%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0031, Overall Accuracy: 6815/10000 (68%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0011, Overall Accuracy: 7312/10000 (73%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0017, Overall Accuracy: 7807/10000 (78%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0016, Overall Accuracy: 8304/10000 (83%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0004, Overall Accuracy: 8802/10000 (88%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0027, Overall Accuracy: 9297/10000 (93%)\n",
      "\n",
      "\n",
      "Test dataset: Overall Loss: 0.0051, Overall Accuracy: 9780/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,2):\n",
    "    train(model, device, train_dataloader,optimizer,epoch)\n",
    "    test(model,device,test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635471c",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d345b0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x236340d36d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "plt.imshow(sample_data[0][0],cmap='gray',interpolation='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8a1e1444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is: 3\n"
     ]
    }
   ],
   "source": [
    "sample_data = sample_data.to(\"cuda\")\n",
    "print(f\"Model prediction is: {model(sample_data).data.max(1)[1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f9072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
