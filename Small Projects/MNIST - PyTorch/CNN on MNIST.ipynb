{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b289fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a23225f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if gpu is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0e5c2",
   "metadata": {},
   "source": [
    "# step1: define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29de132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use torch.nn module to define a model with \n",
    "# 1) two convolutional layers with output channel 16, 32 each with kernel 3 and stride 1\n",
    "# 2) relu activation after each convolutional layer\n",
    "# 3) max pooling with kernel 2\n",
    "# 4) a dropout layer with rate 10%\n",
    "# 5) a flattern layer\n",
    "# 6) a dense layer with 64 neurons\n",
    "# 7) a relu activation\n",
    "# 8) a drop out layer with rate 25%\n",
    "# 9) a dense layer with 10 neurons\n",
    "# 10) use softmax activation\n",
    "\n",
    "class myConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1) # can we choose what features?\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.do1 = nn.Dropout2d(0.1)\n",
    "        self.do2 = nn.Dropout(0.25)\n",
    "        self.lf1 = nn.Linear(4608, 64) # have to calculate the input size by hand?\n",
    "        self.lf2 = nn.Linear(64,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = self.do1(x)\n",
    "        x = torch.flatten(x,1) # not flatten the batch channel?\n",
    "        x = self.lf1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.do2(x)\n",
    "        x = self.lf2(x)\n",
    "        op = F.log_softmax(x,1) # have to exclude last channel for all functions?\n",
    "        return op              # don't need to specify the number of output classes?\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c26ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing VGG model from scratch\n",
    "class myVGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myVGG,self).__init__()\n",
    "        self.conv11=nn.Conv2d(in_channels=1,out_channels=64,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv12=nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv21=nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv22=nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1)\n",
    "        self.dropout2=nn.Dropout2d(p=0.1)\n",
    "        self.conv31=nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv41=nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3,stride=1,padding=1)\n",
    "        self.dense1=nn.Linear(in_features=12544,out_features=64)\n",
    "        self.dropout3=nn.Dropout(p=0.3)\n",
    "        self.dense2=nn.Linear(in_features=64,out_features=10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.conv11(x)\n",
    "        x = self.conv12(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = self.conv21(x)\n",
    "        x = self.conv22(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = self.conv31(x)\n",
    "        x = F.relu(x)\n",
    "        #x = F.max_pool2d(x,2)\n",
    "        #x = self.conv41(x)\n",
    "        #x = F.max_pool2d(x,2)\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x,1)\n",
    "        #print(x.shape)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense2(x)\n",
    "        output =  F.log_softmax(x,1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2597e",
   "metadata": {},
   "source": [
    "## define the training and testing routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11a1a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad() # set gradients to 0\n",
    "        pred_prob = model(X) # output probabilities for each class?\n",
    "        loss = F.nll_loss(pred_prob, y)\n",
    "        loss.backward() # compute gradient\n",
    "        optim.step() # upedate weights\n",
    "        if b_i % 100 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                    epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                    100. * b_i / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cdf8064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval() # for making inference?\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad(): # not calculating gradients\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction=\"sum\").item() # reduction='sum'?\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "            loss /= len(test_dataloader.dataset)\n",
    "        print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            loss, success, len(test_dataloader.dataset),\n",
    "            100. * success / len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa22ae",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb48187",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "                    datasets.MNIST('W://Study Material/Jupyter Notebook/Small Projects/MNIST - PyTorch',\n",
    "                                      train=True,transform=transforms.Compose([\n",
    "                                          transforms.ToTensor(), transforms.Normalize((0.1302,),\n",
    "                                                                                     (0.3069,))\n",
    "                                      ])), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e16aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "                    datasets.MNIST('W://Study Material/Jupyter Notebook/Small Projects/MNIST - PyTorch',\n",
    "                                      train=False,transform=transforms.Compose([\n",
    "                                          transforms.ToTensor(), transforms.Normalize((0.1302,),\n",
    "                                                                                     (0.3069,))\n",
    "                                      ])), batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b953590",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = myConv()\n",
    "model.cuda()\n",
    "optimizer = optim.Adadelta(model.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87a7c127",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.303052\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.792431\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.310549\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.382622\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.146527\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.317115\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.206768\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.141775\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.318441\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.232236\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.111449\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.183354\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.276811\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.085529\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.052319\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.024532\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.058354\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.473393\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.146337\n",
      "\n",
      "Test dataset: Overall Loss: 0.0058, Overall Accuracy: 9775/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.167430\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.032499\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.076318\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.005108\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.216156\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.144696\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.084582\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.046813\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.158303\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.011128\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.049210\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.008963\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.112576\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.034497\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.015883\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.075145\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.011403\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.011593\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.071688\n",
      "\n",
      "Test dataset: Overall Loss: 0.0045, Overall Accuracy: 9831/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training first model \n",
    "for epoch in range(1,3):\n",
    "    train(model, device, train_dataloader,optimizer,epoch)\n",
    "    test(model,device,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ead06716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.314249\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.510173\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.287056\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.262002\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.280889\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.091063\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.123364\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.156935\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.238600\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.028602\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.059175\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.112710\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.003578\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.003944\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.037057\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.101090\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.035623\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.077262\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.016088\n",
      "\n",
      "Test dataset: Overall Loss: 0.0029, Overall Accuracy: 9839/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.019365\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.116373\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.027635\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.053156\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.037572\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.007602\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.096843\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.010645\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.003586\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.041888\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.061518\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.007177\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.086261\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.024493\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.013796\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.028410\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.005669\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.066204\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.005320\n",
      "\n",
      "Test dataset: Overall Loss: 0.0023, Overall Accuracy: 9901/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training second model \n",
    "model2 = myVGG()\n",
    "model2.cuda()\n",
    "optimizer = optim.Adadelta(model2.parameters(),lr=0.05)\n",
    "for epoch in range(1,3):\n",
    "    train(model2, device, train_dataloader,optimizer,epoch)\n",
    "    test(model2,device,test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635471c",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d345b0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d60845c790>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a=150\n",
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "#b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "plt.imshow(sample_data[a][0],cmap='gray',interpolation='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a1e1444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is: 9\n"
     ]
    }
   ],
   "source": [
    "sample_data = sample_data.to(\"cuda\")\n",
    "print(f\"Model prediction is: {model(sample_data).data.max(1)[1][a]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f9072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
